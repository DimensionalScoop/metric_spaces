{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1e4600-1aa3-4cf2-b956-99f4f93fb24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from uncertainties import ufloat\n",
    "import seaborn as sns\n",
    "from joblib import delayed, Parallel\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import tetrahedron\n",
    "import proj_quality\n",
    "from metric.metric import Euclid\n",
    "\n",
    "import pivot_selection\n",
    "import point_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4457df-2d7d-4cb1-b77a-e59fe7986192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Memory\n",
    "mem = Memory(\"~/.cache/joblib/\", verbose=0)\n",
    "tetrahedron.project_to_2d_euclidean = mem.cache(tetrahedron.project_to_2d_euclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378f6bf4-fdb7-4a6f-b217-b262972784f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = Euclid(2)\n",
    "DEFAULT_SAMPLE_SIZE = 120\n",
    "\n",
    "generators = point_generator.get_generator_dict()\n",
    "piv_selectors = pivot_selection.get_selection_algos(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e292b2f-eeed-4f86-bad1-32af3dad1660",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(0xfeed)\n",
    "points = point_generator.generate_gaussian_points(rng, dim=15, n_samples=100)\n",
    "\n",
    "test_algs = {i:piv_selectors[i] for i in (\n",
    "    \"maximize_dist\",\n",
    "    \"non_central_points\",\n",
    "    \"non_central_points_approx\",\n",
    "    \"hilbert_optimal\",\n",
    "    \"ccs_optimal\",\n",
    "    )}\n",
    "\n",
    "plt.scatter(points[:,0], points[:,1], alpha=0.3, marker=\"+\")\n",
    "\n",
    "for name, piv_func in test_algs.items():\n",
    "    exact = np.array(piv_func(points, rng=rng))\n",
    "    plt.plot(exact[:,0], exact[:,1], label=name)\n",
    "    \n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ff0b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_projections(point_gen:dict, pivot_selector:dict, dims:list, seed=0, errors=\"skip\"):\n",
    "    rv = []\n",
    "    rng = np.random.default_rng(seed)\n",
    "    for dim in dims:\n",
    "        for gen_name, gen_func in point_gen.items():\n",
    "            points = gen_func(dim=dim, rng=rng)\n",
    "            r = proj_quality.get_average_k_nn_dist(points, metric, k=10)\n",
    "            for algo_name, select_pivots in pivot_selector.items():\n",
    "                def doit():\n",
    "                    p0, p1 = select_pivots(points, rng=rng)\n",
    "                    points_p = tetrahedron.project_to_2d_euclidean(points, p0, p1, metric)\n",
    "                    rv.append(dict(\n",
    "                        dim=dim,\n",
    "                        dataset=gen_name,\n",
    "                        algorithm=algo_name,\n",
    "                        mean_candidate_set_size=proj_quality.candidate_set_size(points_p,r,metric),\n",
    "                        hilbert_quality=proj_quality.hilbert_quality(points_p,r),\n",
    "                        note=\"\"\n",
    "                    ))\n",
    "                if errors == \"skip\":\n",
    "                    try:\n",
    "                        doit()\n",
    "                    except:\n",
    "                        rv.append(dict(\n",
    "                            dim=dim,\n",
    "                            dataset=gen_name,\n",
    "                            algorithm=algo_name,\n",
    "                            mean_candidate_set_size=-1,\n",
    "                            hilbert_quality=-1,\n",
    "                            note=\"failed\",\n",
    "                        ))\n",
    "                elif errors == \"raise\":\n",
    "                    doit()\n",
    "                else:\n",
    "                    raise NotImplementedError()\n",
    "    rv = pd.DataFrame(rv)\n",
    "    return rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92d1336-6f92-4979-80b0-aaf26755ec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_runs(results):\n",
    "    def to_ufloat(data):\n",
    "        return ufloat(np.mean(data), np.std(data,ddof=1))\n",
    "    rv = results.drop(columns=\"run\").groupby(\"algorithm\").agg(to_ufloat)\n",
    "    return rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca3f7b0-dace-4d9a-8ee2-915b5c9372ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d8910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = range(2, 17)\n",
    "sampels = range(16)\n",
    "\n",
    "for global_run in range(1000):\n",
    "    print(f\"starting global run {global_run}â€¦\")\n",
    "    def run(run_id):\n",
    "        r = compare_projections(generators, algs, dims, seed=global_run*1000+run_id)\n",
    "        r[\"run\"] = run_id\n",
    "        return r\n",
    "    \n",
    "    jobs = [delayed(run)(run_id) for run_id in sampels]\n",
    "    results = pd.concat(Parallel(n_jobs=16, verbose=11)(jobs))\n",
    "    results.to_csv(f\"./results_total_{global_run}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d06d6ef-1571-456e-bf0a-690a549f6130",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"./results_long.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e12397-6568-48d9-ad70-81bf2c916f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4816489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize(df):\n",
    "    df = df[df.note != \"failed\"].copy()\n",
    "    random = df.query(\"algorithm == 'random'\")\n",
    "    df.hilbert_quality -= random.hilbert_quality.mean()\n",
    "\n",
    "    best = df.query(\"algorithm == 'optimized_hq'\")\n",
    "    if len(best):\n",
    "        df.hilbert_quality /= best.hilbert_quality.mean()\n",
    "    df.mean_candidate_set_size /= random.mean_candidate_set_size.mean()\n",
    "    return df\n",
    "\n",
    "drop_ratio = 0.25\n",
    "measure = \"hilbert_quality\"\n",
    "def _drop_worst(group:pd.DataFrame):\n",
    "    thresh = group[measure].quantile(drop_ratio)\n",
    "    return group[group[measure] > thresh]\n",
    "    \n",
    "normalized_res = results.groupby([\"dim\",\"dataset\", \"algorithm\"]).apply(_drop_worst, include_groups=False)\n",
    "normalized_res = normalized_res.reset_index()#.drop(columns=\"level_2\")\n",
    "assert np.allclose(1-len(normalized_res)/len(results), drop_ratio,atol=0.05)\n",
    "\n",
    "normalized_res = results.groupby([\"dataset\",\"dim\"]).apply(_normalize, include_groups=False)\n",
    "normalized_res = normalized_res.reset_index().drop(columns=\"level_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d912b607-518a-4709-989b-945b43c3b4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "measure = \"hilbert_quality\"\n",
    "def _discard_if_worse_than_random(group):\n",
    "    group = group.reset_index(drop=True)\n",
    "    mean = group.groupby(\"algorithm\")[measure].mean()\n",
    "    std =  group.groupby(\"algorithm\")[measure].std()\n",
    "    thresh = mean[\"random\"] - std[\"random\"]\n",
    "    discard = mean.index[mean < thresh]\n",
    "    return group[~group.algorithm.isin(discard)]\n",
    "\n",
    "# drop stuff worse than random\n",
    "normalized_res = normalized_res.groupby([\"dim\",\"dataset\"]).apply(_discard_if_worse_than_random, include_groups=False)#.reset_index(drop=False)\n",
    "normalized_res = normalized_res.reset_index().drop(columns=\"level_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4620b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(normalized_res[normalized_res.algorithm.isin([\"non_central_improved\",\"non_central_points_approx\", \"non_central_points_approx_log\"])]\n",
    "                  , col=\"dataset\", hue=\"algorithm\", col_wrap=2,\n",
    "                 )\n",
    "g.map(sns.lineplot, \"dim\", \"hilbert_quality\",errorbar=\"ci\")#.set(ylim=(-0.2,1.1))\n",
    "g.add_legend()\n",
    "g.savefig(\"fig/hilbert_quality_200.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc965c0-ef3e-442e-a8d5-7dce45d1fb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(normalized_res[normalized_res.algorithm.isin([\"non_central_improved\",\"non_central_points_approx\"])]\n",
    "                  , col=\"dataset\", hue=\"algorithm\", col_wrap=2)\n",
    "g.map(sns.lineplot, \"dim\", \"mean_candidate_set_size\")\n",
    "g.add_legend()\n",
    "g.savefig(\"fig/candidate_set_size_200.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc8aae1-d3a1-45e8-a37d-14aad615398e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    maximize_dist=max_dist_points,\n",
    "    non_central_points=two_least_central,\n",
    "    non_central_improved=low_centrality_and_far_away,\n",
    "    non_central_points_approx=two_least_central_heuristically,\n",
    "    non_central_points_approx_log=lambda *args,**kwargs:two_least_central_heuristically(\n",
    "        *args,**kwargs, budget=lambda x:np.log(x)),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fc0fbb-ffb6-47dc-a5ec-ae41593cc73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(results, col=\"dataset\", hue=\"algorithm\", col_wrap=2,\n",
    "                 )\n",
    "g.map(sns.lineplot, \"dim\", \"hilbert_quality\",errorbar=\"ci\")#.set(ylim=(-0.2,1.1))\n",
    "g.add_legend()\n",
    "g.savefig(\"fig/hilbert_quality_200.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19c7745-37f8-409e-af5c-f89fcb5b42fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"optimized-comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abeb39f-84c2-467b-af0e-2839a8b279aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "piv_selectors.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3af729-0a14-451f-9da9-2ead93c48350",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = range(2,17)\n",
    "sampels = range(16)\n",
    "\n",
    "test_algs = {i:piv_selectors[i] for i in\n",
    "    [\"random\",\n",
    "    \"maximize_dist\",\n",
    "    \"non_central_points\",  ] \n",
    "            }\n",
    "\n",
    "def run(run_id):\n",
    "    r = compare_projections(generators, test_algs, dims, seed=100+run_id, errors=\"raise\")\n",
    "    r[\"run\"] = run_id\n",
    "    return r\n",
    "\n",
    "jobs = [delayed(run)(run_id) for run_id in sampels]\n",
    "results = pd.concat(Parallel(n_jobs=1, verbose=11)(jobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f90b87-8b61-4b72-8845-04feba7b114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df3e6ec-7f40-4ac9-b98c-c5bbdd527ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cc3911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hilbert_qual(ps):\n",
    "    part = proj_quality.HilbertPartitioner(ps, ten_nn_dist)\n",
    "    l, r = part.get_partitions(ps)\n",
    "\n",
    "    plt.scatter(*ps.T)\n",
    "    plt.scatter(*ps[l].T, marker=\"+\", label=\"left partition\")\n",
    "    plt.scatter(*ps[r].T, marker=\"x\", label=\"right partition\")\n",
    "    \n",
    "    hq = part.hyperplane_quality(ps)\n",
    "    css = proj_quality.candidate_set_size(ps,ten_nn_dist, Euclid(2))\n",
    "    plt.title(f\"partitioning {hq:.0%} points\\nmean CSS {css:.0f}\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f99652",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2,2,1)\n",
    "plot_hilbert_qual(points_p)\n",
    "plt.subplot(2,2,2)\n",
    "plot_hilbert_qual(points_p_max_dist)\n",
    "plt.subplot(2,2,3)\n",
    "plot_hilbert_qual(points_p_min_dist)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
